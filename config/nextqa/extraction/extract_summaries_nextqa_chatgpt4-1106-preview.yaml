mode: eval # (train / eval / test)
experiment_path: "./experiments/EXP_NQA_L3_3"

# Path to the secret environment variables (e.g. API keys)
secret_env: "./.env"


# Setup of the logger
logger:
  # CRITICAL = 50, ERROR = 40, WARNING = 30, INFO = 20, DEBUG = 10, NOTSET = 0
  level: 10
  format: "%(asctime)s - %(levelname)s - %(module)s - %(message)s"

# Specify the directory of the videos
videos_path: "./data/nextqa/NExTVideo"

# Specify the directory of the tasks (questions and options)
tasks_path: "./data/nextqa/val.csv"

# Specify the directory of the answers (i.e. the ground truths)
answers_path: "./data/nextqa/val.csv"

# Specify the directory of a previous experiment to go on with the inferences
# (if null, start from scratch)
resume_path: null

# to save monetary costs, we only use a 10% subset of the videos
# please refer to the script sample_subset_indices.py to generate the indices (we used seed 42)
subset_indices: [3, 4, 13, 17, 26, 29, 45, 53, 58, 64, 76, 79, 94, 120, 159, 172, 193, 200, 203, 204, 217, 226, 244, 257, 260, 262, 295, 313, 317, 325, 326, 342, 355, 361, 363, 375, 378, 385, 410, 414, 416, 443, 458, 465, 468, 474, 479, 480, 482, 496, 499, 520, 524, 542, 552, 556, 561, 563, 566, 568, 569, 573, 580, 581, 584, 594, 596, 600, 616, 617, 645, 653, 656, 671, 694, 701, 712, 744, 756, 759, 764, 765, 767, 772, 792, 794, 802, 806, 819, 827, 837, 839, 842, 848, 871, 873, 876, 885, 888, 895, 898, 912, 913, 916, 938, 944, 945, 947, 982, 991, 1020, 1022, 1030, 1042, 1044, 1051, 1071, 1072, 1083, 1085, 1100, 1133, 1143, 1148, 1170, 1207, 1208, 1213, 1221, 1252, 1266, 1268, 1273, 1289, 1292, 1295, 1304, 1307, 1310, 1323, 1331, 1332, 1338, 1348, 1352, 1362, 1390, 1401, 1425, 1450, 1462, 1463, 1469, 1503, 1522, 1555, 1557, 1558, 1559, 1569, 1571, 1575, 1582, 1592, 1614, 1628, 1629, 1631, 1634, 1647, 1667, 1673, 1716, 1720, 1721, 1725, 1728, 1735, 1741, 1746, 1750, 1751, 1755, 1763, 1783, 1791, 1792, 1796, 1799, 1804, 1805, 1822, 1826, 1828, 1840, 1864, 1866, 1875, 1876, 1885, 1905, 1907, 1924, 1926, 1936, 1939, 1949, 1955, 1961, 1973, 1990, 1998, 2001, 2005, 2006, 2016, 2020, 2030, 2032, 2036, 2044, 2046, 2057, 2059, 2086, 2136, 2139, 2145, 2152, 2157, 2162, 2166, 2169, 2171, 2172, 2174, 2175, 2185, 2187, 2193, 2211, 2231, 2253, 2266, 2276, 2277, 2278, 2281, 2282, 2286, 2308, 2324, 2334, 2336, 2350, 2370, 2401, 2404, 2422, 2430, 2444, 2452, 2457, 2460, 2465, 2478, 2489, 2500, 2519, 2525, 2533, 2542, 2553, 2569, 2571, 2573, 2577, 2584, 2590, 2591, 2639, 2655, 2656, 2672, 2689, 2704, 2706, 2712, 2721, 2732, 2745, 2757, 2769, 2779, 2786, 2787, 2808, 2817, 2831, 2859, 2864, 2876, 2897, 2902, 2910, 2931, 2940, 2962, 2965, 2973, 2983, 2987, 2988, 3024, 3027, 3032, 3059, 3063, 3100, 3104, 3105, 3108, 3112, 3113, 3126, 3133, 3145, 3152, 3155, 3198, 3241, 3242, 3264, 3268, 3271, 3273, 3274, 3285, 3286, 3296, 3298, 3307, 3326, 3329, 3334, 3345, 3367, 3372, 3377, 3395, 3434, 3436, 3444, 3456, 3458, 3462, 3465, 3466, 3469, 3482, 3503, 3509, 3520, 3521, 3531, 3533, 3559, 3561, 3572, 3574, 3588, 3589, 3619, 3622, 3630, 3675, 3679, 3698, 3714, 3716, 3727, 3730, 3745, 3754, 3758, 3763, 3770, 3786, 3789, 3809, 3825, 3834, 3855, 3872, 3874, 3876, 3892, 3893, 3905, 3943, 3976, 3978, 3981, 3986, 4002, 4042, 4049, 4089, 4100, 4119, 4139, 4140, 4144, 4154, 4158, 4160, 4173, 4174, 4187, 4189, 4211, 4222, 4239, 4259, 4282, 4322, 4333, 4334, 4344, 4350, 4363, 4375, 4392, 4393, 4398, 4404, 4410, 4415, 4417, 4418, 4428, 4441, 4450, 4464, 4467, 4469, 4488, 4503, 4508, 4519, 4522, 4526, 4532, 4537, 4541, 4543, 4552, 4562, 4586, 4594, 4597, 4598, 4612, 4630, 4646, 4647, 4662, 4666, 4677, 4715, 4720, 4723, 4729, 4742, 4744, 4751, 4756, 4771, 4780, 4781, 4785, 4788, 4819, 4820, 4827, 4837, 4870, 4874, 4881, 4885, 4887, 4906, 4931, 4945, 4954, 4966, 4969, 4988, 4990]


# Specify the sample rate to be used to load the videos (in fps)
sample_rate: 1.0

# specify whether to iterate through the videos or tasks (use false for reasoning and true for extraction)
# we iterate through videos here to avoid factorized summarization costs
iterate_through_videos: true

# random seed
random_seed: 42
reset_seed_for_each_function: true
reset_seed_for_each_video: true


# Setup of the VideoINSTA controller
controller:
  max_iterations: 1


# Specify the params of the state
state:
  spatial_clip_state:
    use_action_captions: false
    use_action_captions_summary: false
    use_object_detections: false
    use_object_detections_summary: false
  temporal_clip_state:
    use_foreground: false
    use_relevance: false
    use_salience: false
    use_temporal_grounding_summary: false
  lexical_representation: "sections"

# Specify the params of the operations that have params
operations:
  # Specify the number of splits for the initial root split
  split:
    class: "DPCKNNSplit"
    params:
      num_clusters: 4
      k: 5
      clip_model_name: "openai/clip-vit-large-patch14"
      use_density_minimum_as_border: true
      reset_seed: true
  wait:
    class: "Wait"
    params: { }
  merge:
    class: "Merge"
    params: { }

  derive_clip_states:
    class: "DeriveClipStates"
    params:
      derive_action_captions: true
      derive_action_captions_summary: true
      total_num_words_action_captions_summary: 500
      min_num_words_action_captions_summary: 20
      derive_object_detections: true
      derive_object_detections_summary: true
      total_num_words_object_detections_summary: 500
      min_num_words_object_detections_summary: 20
      derive_temporal_grounding: false
      derive_temporal_grounding_summary: false
      total_num_words_temporal_grounding_summary: 500
      min_num_words_temporal_grounding_summary: 20
      # -1 means that we do not normalize, we just take the total number of words as contingent for the video,
      # no matter how long the video is (e.g. for 4-second videos as well as for 180-second videos)
      normalization_video_length: -1

  derive_root_clip_state:
    class: "DeriveRootClipState"
    params:
      derive_action_captions: true
      derive_action_captions_summary: false
      num_words_action_captions_summary: 500
      min_num_words_action_captions_summary: 20
      derive_object_detections: true
      derive_object_detections_summary: false
      num_words_object_detections_summary: 500
      min_num_words_object_detections_summary: 20
      derive_temporal_grounding: false
      derive_temporal_grounding_summary: false
      num_words_temporal_grounding_summary: 500
      min_num_words_temporal_grounding_summary: 20
      # -1 means that we do not normalize, we just take the total number of words as contingent for the video,
      # no matter how long the video is (e.g. for 4-second videos as well as for 180-second videos)
      normalization_video_length: -1

  ratings:
    class: "NoRatings"
    params: { }

  decision:
    class: "NoDecision"
    params: { }

  conclusion:
    class: "NoConclusion"
    params: { }

api:
  load_models_only_when_needed: true

  get_object_detections_from_video_clip_and_text:
    config_file: "./toolbox/GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py"
    checkpoint: "./toolbox/GroundingDINO/weights/groundingdino_swinb_cogcoor.pth"
    box_threshold: 0.0
    text_threshold: 0.0
    cuda: true

  get_unspecific_objects_from_video_clip:
    pre_inferred_object_detections_path: "./perceptive_data/object_detections_nextqa_cogagent-vqa-hf_0.0_3_objects.json"
    frame_prompt: "You are given an image. Your task is to identify the three main objects in the image. 
Only identify the objects that are clearly visible in the image and unambiguously recognizable. 
Do not speculate about objects that are not clearly visible. 
Find short and precise names for the objects and avoid paraphrasing them. 
Provide an enumerated list with the three main objects you can clearly identify in the image. 
\nList of objects:\n"
    model_id: "THUDM/cogagent-vqa-hf"
    tokenizer_id: "lmsys/vicuna-7b-v1.5"
    device: "cuda"
    precision: "torch.float16"
    quantization: false
    max_new_tokens: 512
    do_sample: false # greedy
    temperature: 0.0 # will be ignored since do_sample is false
    num_objects_per_frame: 3

  get_specific_objects_from_video_clip:
    # leverages get_completion_from_text API function
    # leverages get_object_detections_from_video_clip_and_text API function
    prompt_template: "Given:
\n- a question about a video: {question}
\n- answer option 0: {option_0}
\n- answer option 1: {option_1}
\n- answer option 2: {option_2}
\n- answer option 3: {option_3}
\n- answer option 4: {option_4}
\n\nTask:
\nI want to identify objects based on that question and its answer options that should be given special attention while watching the video.
\nThese specific objects should be detectable by an object detection model.
\nPlease provide an enumerated list of these objects without any speculation about the video content.
\nMake sure that each item of the list only contains the name of the object.
\nDo not provide general categories or descriptions.
\nDo not provide any additional information, explanations or examples.
\n\n"
    completion_start: "Terms of objects:\n"
    max_new_tokens: 512
    temperature: null
    replace_c: false

  get_action_captions_from_video_clip:
    # TODO probably we will also have to try with the image descriptions
    pre_inferred_action_captions_path: "./perceptive_data/action_captions_nextqa_cogagent-vqa-hf_0.0_10_words_events.json"
    model_name: "LaViLa"
    modelzoo_dir_path: "./toolbox/lavila_video_captioner/modelzoo"
    checkpoint_download_url: "https://dl.fbaipublicfiles.com/lavila/checkpoints/narrator"
    checkpoint: "ckpt_base.pt" # fair version of LaViLa that has not seen the Egoschema clips
    resample_rate: 4.0
    interval_in_seconds: 1
    # seems to be the best temperature for LaViLa so far,
    # even leads to better results than using the pre-inferrred action captions from LLoVi
    temperature: 0.0001 # one of the best in ablation together with 0.1, but this way we have more diversity for sampling
    top_p: 0.95 # like in LLoVi
    max_new_tokens: 256 # whatever, just not too small, will be small fraction anyway
    num_return_sequences: 5 # like in LLoVi
    early_stopping: true # faster
    num_seg: 4
    cuda: true

  get_temporal_grounding_from_video_clip_and_text:
    config_dir: "./toolbox/UniVTG/tmp"
    checkpoint: "./toolbox/UniVTG/results/omni/finetuned/model_best.ckpt"
    clip_model_version: "ViT-B/32"
    output_feat_size: 512
    half_precision: false
    jit: false
    resize: 224
    gpu_id: 0
    top_k_intervals: 2

  get_summary_from_noisy_perceptive_data:
    # here we want to infer the summaries
    pre_inferred_summaries_path: null
    # similar summarization prompt like LLoVi
    action_caption_prompt_template: "You are given some language descriptions of a first person view video. 
The video is {length} seconds long. Each sentence describes a 1.0s clip. The descriptions are sequential and non-overlapping 
which cover the whole video exactly. Here are the descriptions: {interval_text}.\n Please give me a {words} words summary. 
When doing summarization, remember that your summary will be used to answer this multiple choice question: {question}"
    action_caption_completion_start: ""
    object_detection_prompt_template: "You are given a list of the most eye-catching objects that were detected in each 
frame of a video clip using a visual large language model. The list appears in the temporal order of the frames. The 
video is {length} seconds long. Each sentence describes the objects of a 1.0s clip. The object detections are 
sequential and non-overlapping which cover the whole video exactly. Here are the object detections:
\n\n{interval_text}.
\n\nPlease give me a {words} words summary of these object detections. When doing summarization, remember that your 
summary will be used to answer this multiple choice question: {question}"
    object_detection_completion_start: ""
    # difference to chat: added instruction for providing a readable summary without special formatting (which llama3 tends to do)
    temporal_grounding_prompt_template: "# Summarization of temporal data
\n\nHi there! You are given temporal information of a video. 
This information has been grounded on a question and five answer options of a video question answering task respectively. 
This grounding was done by a temporal grounding model. The available information consists of a foreground, 
relevance and saliency for the question and each option regarding the frame features in a video respectively.
\n\n## Foreground
\n\nThe foreground represents the percentage of frames that indicate the foreground of the clip regarding the texts of 
the question and each option respectively. For example, given the text of the option X, a value of 100% would mean that 
all frames of the clip are related to the option X and a value of 0% would mean that none of the frames are related to 
the option X.
\n\n## Relevance
\n\nThe relevance represents the percentage of frames that are relevant regarding the texts of the question and each 
option respectively. For example, given the text of the question, a value of 100% would indicate that all frames of the 
clip are relevant to the question, while a value of 0% would mean that no frame of the clip is relevant to the question.
\n\n## Salience
\n\nThe salience represents the mean percentage of the salience of each frame regarding the texts of the question and 
each option respectively. For example, given the text of option X, a value close to 100% would mean that almost all 
frames are very salient regarding option X, while a value close to 0% would mean that almost all frames are not salient 
regarding option X.
\n\n## Here are the temporal groundings
\n\n{interval_text}
\n\n## Here is the question
\n\n{question}
\n\n## Here are the options
\n\n    A) {option_0}
\n    B) {option_1}
\n    C) {option_2}
\n    D) {option_3}
\n    E) {option_4}
\n\n## Now it is your turn
\n\nPlease give me a 100 words summary of these temporal groundings. Please use the foreground, the relevance and the salience 
in your summary. Remember that your summary will be used to answer the given multiple choice question.
\n\n"
    temporal_grounding_completion_start: ""
    max_new_tokens: 2048
    temperature: 1.0
    no_recursion: false
    interval_span: 10000
    remove_linebreaks: false
    replace_c: false

  get_completion_from_text:
    llm_class: "OpenAILLM"
    llm_name: "gpt-4-1106-preview"
    temperature: 0.0 # the default LLM temperature is 0 for the QA reasoning
    frequency_penalty: 0.0
    presence_penalty: 0.0
    max_new_tokens: 4096
    api_type: "chat"
    system_prompt: "You are a helpful expert in video analysis."  # like in LLoVi, but more generic
    use_seed: false # like in LLoVi, let's catch some big fish